{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3b12dbd",
      "metadata": {},
      "source": [
        "# <u>**Multi-Head Attention**</u>\n",
        "\n",
        "Multi-head attention (Vaswani et al., 2017) builds on the attention mechanism introduced for neural machine translation (Bahdanau et al., 2015). Instead of a single attention map, it learns multiple attention \"heads\" in parallel, allowing the model to capture different types of relationships across the same sequence.\n",
        "\n",
        "**Unmasked vs. masked:**\n",
        "- **Unmasked multi-head attention** allows each token to attend to all tokens (encoder-style).\n",
        "- **Masked multi-head attention** uses a causal mask so tokens cannot look ahead (decoder-style).\n",
        "\n",
        "In practice, each head specializes in different alignment patterns (e.g., syntax, locality, long-range dependencies), and their outputs are combined to form a richer representation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44667988",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n",
        "\n",
        "Multi-head attention is defined as:\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O$$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$\n",
        "\n",
        "**Masked version (causal):**\n",
        "\n",
        "$$\\text{Attention}(Q, K, V, M) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)V$$\n",
        "\n",
        "Here, $M$ is an additive mask with $-\\infty$ on positions that should be blocked.\n",
        "\n",
        "### Visual Intuition\n",
        "\n",
        "![Scaled Dot-Product Attention](./assets/self_attention_diagram.png)\n",
        "\n",
        "![Multi-Head Attention](./assets/multihead_attention_diagram.png)\n",
        "\n",
        "Each head performs scaled dot-product attention on its own learned projections, and the results are concatenated and linearly mixed to produce the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585d58f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d44e55",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model: int, h: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Our implementation of Multi-Headed Scaled Dot-Product Attention. \n",
        "\n",
        "        Args:\n",
        "            d_model (int): Model embedding dimension.\n",
        "            h (int): Number of attention heads.\n",
        "            dropout: float\n",
        "            \n",
        "        Context: \n",
        "            d_model = size of embedding vector\n",
        "            h = number of heads\n",
        "            d_k = dimension of each head (corr: width)\n",
        "        \n",
        "        Just to keep consistency with the maths in the paper\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model              # Also equivalent to h * d_k\n",
        "        self.h = h\n",
        "        assert d_model % h == 0\n",
        "        \n",
        "        self.d_k = d_model // h             # Also equivalent to d_v in paper\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "        \n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        \n",
        "    def split_heads(self, x, batch_size: int):\n",
        "        \"\"\"\n",
        "        Split the model dimension into multiple heads.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor [Q', K' V'] of shape (batch, seq_len, d_model).\n",
        "            batch_size (int): Batch size for reshaping.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of shape (batch, n_heads, seq_len, d_k).\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        x = x.reshape(batch_size, seq_len, self.h, self.d_k)\n",
        "        return x.permute(0, 2, 1, 3)          \n",
        "    \n",
        "    def compute_attn(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention for each group\n",
        "        of split heads (e.g., q1, k1, v1, ...).\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor of shape (batch, h, seq_len, d_k).\n",
        "            key (torch.Tensor): Key tensor of shape (batch, h, seq_len, d_k).\n",
        "            value (torch.Tensor): Value tensor of shape (batch, h, seq_len, d_k).\n",
        "            mask (torch.Tensor, optional): Attention mask broadcastable to\n",
        "                (batch, h, seq_len, seq_len). Positions with 0 are masked.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Attention output of shape (batch, h, seq_len, d_k).\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        return torch.matmul(attn_weights, value)\n",
        "    \n",
        "    def concat_heads(self, x, batch_size: int):\n",
        "        \"\"\"\n",
        "        Concatenate attention heads back into the model dimension (tensor H).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor of shape (batch, n_heads, seq_len, head_dim).\n",
        "            batch_size (int): Batch size for reshaping.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of shape (batch, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        return x.view(batch_size, -1, self.d_model)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q (torch.Tensor): Query tensor of shape (batch, seq_len, d_model).\n",
        "            K (torch.Tensor): Key tensor of shape (batch, seq_len, d_model).\n",
        "            V (torch.Tensor): Value tensor of shape (batch, seq_len, d_model).\n",
        "            mask (torch.Tensor, optional): Attention mask broadcastable to\n",
        "                (batch, n_heads, seq_len, seq_len). Positions with 0 are masked.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        \n",
        "        query = self.split_heads(self.W_q(Q), batch_size)\n",
        "        key = self.split_heads(self.W_k(K), batch_size)\n",
        "        value = self.split_heads(self.W_v(V), batch_size)\n",
        "        \n",
        "        attention = self.compute_attn(query, key, value, mask)\n",
        "        \n",
        "        output = self.concat_heads(attention, batch_size)\n",
        "        \n",
        "        return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aefcbb69",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). \n",
        "   **Attention Is All You Need.** *Advances in Neural Information Processing Systems, 30.* \n",
        "   [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db69bdb",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
